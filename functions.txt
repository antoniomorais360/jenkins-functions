
export ownip=$(hostname -I | awk '{print $1}')

function aws_iid_tagname() {
	aws_iid_tagname=$(aws ec2 describe-instances --filters "Name=tag:Name,Values=$1" \
	--output text --query 'Reservations[*].Instances[*].InstanceId')
	echo $aws_iid_tagname
}

function aws_running_instances () {

aws ec2 describe-instances  \
	--filter "Name=instance-state-name,Values=running" \
	--query "Reservations[*].Instances[*].[InstanceId, PrivateIpAddress, PublicIpAddress, InstanceType, State.Name, Tags[?Key=='Name'].Value|[0],Tags[?Key=='SO'].Value|[0], Platform]" \
	--output text 

}

function aws_elastic_ips () {

aws ec2 describe-addresses \
	--query 'Addresses[].{ instanceId: InstanceId, publicIp: PublicIp, allocationId: AllocationId, associationId: AssociationId, zone: NetworkBorderGroup }' \
	--output text 

}

function run_aws_variables_instances() {

	ret=$?
	if [ $ret -ne 0 ]; then
		echo Atualizando aws-variaveis-instancias
		curl -X POST http://$USERNAME:$TOKEN@localhost:8080/job/aws-variaveis-instancias/build?token=$TOKENJOB
		exit 1
	fi

}

function check_cpu () {
#Requirement: apt install -y sysstat (mpstat)
#Example: 	check_cpu ip linux_user 
#			check_cpu 192.168.0.1 ubuntu
 
ipnumber=$1
linux_user=$2
linux_user=${linux_user:-ubuntu}


#CPU variables
test_duration=5


	cpu_idle=$(sudo -u $linux_user ssh -o StrictHostKeyChecking=no $ipnumber mpstat $test_duration 1 | tail -1 |awk '{print int($12)}')
	cpuusage=$(echo 100 - $cpu_idle | bc)
	cpuusage=$(echo ${cpuusage%,*})
	
	check_cpu=$cpuusage
	
	printf "$1 is using $cpuusage%% of CPU\n"

}

function ping_url () {
#Example: ping_url "My Site Friendly Name" https://mysite.com

sitename=$1
url2check=$2


#PING url variables
connecttimeout=20

echo 
echo Testing $1

response_code=`curl --insecure --write-out %{http_code} --silent --connect-timeout $connecttimeout --no-keepalive --output /dev/null  $url2check`

ping_url=$response_code 
  
  if [ "$response_code" != "200" ] && [ "$response_code" != "404" ] && [ "$response_code" != "401" ] && [ "$response_code" != "301" ] ; then

			printf "$1 is *** OFFLINE *** \n"
            false
  else
  	printf "$1 is ONLINE ($response_code)\n"
    echo ""
  fi;

}

function check_hdd_free_space() {

#Example: check_hdd_free_space "Wordpress" 192.168.0.1 /

instancename=$1
hostip=$2
pathvolume=$3


echo $instancename - $hostip

if [ "$hostip" = "localhost" ]; then
	
	check_hdd_free_space=$(df $pathvolume | grep -o "[0-9][0-9]"% |  grep -o "[0-9][0-9]")
else	
	check_hdd_free_space=$(sudo -u ubuntu ssh -o StrictHostKeyChecking=no -o LogLevel=error $2 df $pathvolume | grep -o "[0-9][0-9]"% |  grep -o "[0-9][0-9]" )
	
fi;	

printf "$check_hdd_free_space%%\n"

}

function check_ssl_expire() {
site=$1
PORT="443"

date --date="$(echo | openssl s_client -servername $site -connect $site:$PORT  2>/dev/null \
	| openssl x509 -noout -enddate | awk -F '=' '{print $NF}' )" --iso-8601
	
}

function run_hdd_clean_up () {

ipnumber=$1
linux_user=$2
linux_user=${linux_user:-ubuntu}

cmd_run="sudo apt-get -y clean && sudo apt-get -y autoclean && sudo apt-get -y autoremove"

if [ "$ipnumber" == "$ownip" ]  
    then
		eval $cmd_run
	else
      sudo -u $linux_user ssh -o LogLevel=error $ipnumber $cmd_run  
 fi
}

function check_hdd_biggest () {

#Ex: check_hdd_biggest 

ipnumber=$1
linux_user=$2
linux_user=${linux_user:-ubuntu}

findfiles="sudo find / -not -path '/mnt/*' -not -path '/proc/*' \
		-not -path '/swapfile/*' -size +20M  -type f  -exec du -h  {} + | sort -r -h"
findpaths="sudo du -hs /*  --exclude=/mnt/*  --exclude=/proc/* | sort -rh | head -10"

if [ "$ipnumber" == "$ownip" ]  
    then
		eval $findfiles
		eval $findpaths
	else
      sudo -u $linux_user ssh -o LogLevel=error $ipnumber $findfiles
      sudo -u $linux_user ssh -o LogLevel=error $ipnumber $findpaths
 fi
    
}

function run_hdd_deletions () {

folder=$1
filename=$2
days_expiration=$3

	sudo find $folder -type f -mtime +$(days_expiration) -name $filename -exec rm -f {} \;

}

function aws_expand_hdd () {

#to be used after resize image in instance-state-name
#Ex: aws_expand_hdd nvme0n1 nvme0n1p1
dev_name=$1
dev_partition=$2

	sudo apt install cloud-guest-utils
	lsblk
	df -h
	sudo growpart /dev/$dev_name 1
	sudo resize2fs /dev/$dev_partition

}

function run_reboot () {

ipnumber=$1
linux_user=$2
linux_user=${linux_user:-ubuntu}

cmd_run="sudo reboot"

if [ "$ipnumber" == "$ownip" ]  
    then
		sudo $cmd_run
	else
      sudo -u $linux_user ssh -o LogLevel=error $ipnumber $cmd_run
 fi
}

function time_download () {
#usage time_download https://mysite.com/myfile.txt 5
#$1 file, $2 times to test or leave empty for 10 times

file=$1
times=${times:-10}

average=0
for i in $(seq $times); do
	newvalue=$(curl -o /dev/null --http1.1 -sS -w %{time_total}\\n  $file ) 
	newvalue=$(echo $newvalue |  sed -e "s/,/./g")
	average=$(echo "scale=3;$average + $newvalue" | bc)
done
echo $file
echo "scale=3;$average / $times" | bc

}

function time_load_site (){

site=$1

time_load_site=$(time wget --debug --verbose --directory-prefix=/tmp -erobots=off --output-file=/tmp/logfile.txt --page-requisites --no-cache --delete-after --level=inf --span-hosts $site)

echo $time_load_site


# Explained
#https://www.computerhope.com/unix/wget.htm
#wget \
#	-r --recursive \ # Download the whole site. \ 
#    -p --page-requisites \ # Get all assets/elements (CSS/JS/images).
#	--spider stops wget from downloading the page.
#	-nd, short for --no-directories, prevents wget from creating a hierarchy of directories on your server.
#   -H --span-hosts \ # Include necessary assets from offsite as well.
#    --convert-links \ # Update links to still work in the static version.
#    --domains yoursite.com \ # Do not follow links outside this domain.
#	-nv, --no-verbose, stops wget from outputting extra information that is unnecessary for identifying broken links.
#    --no-parent \ # Don't follow links outside the directory you pass in.
#         yoursite.com/whatever/path # The URL to download
#	-l 1 is short for --level. By default, wget crawls up to five levels deep from the initial URL, but here we set it to one.
#	-w 2, short for --wait, instructs wget to wait 2 seconds between requests to avoid bombarding the server, minimizing any performance impact.

}

function site_links () {

site=$1
wget --spider --recursive --no-verbose --output-file=wgetlog.txt $site
sed -n "s@.\+ URL:\([^ ]\+\) .\+@\1@p" wgetlog.txt | sed "s@&@\&amp;@" > sedlog.txt
awk '!seen[$0]++' sedlog.txt > /dev/null #remove duplicate lines
cat sedlog.txt | sort
}





